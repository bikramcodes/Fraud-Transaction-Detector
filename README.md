# ğŸ¦ ATM Fraud Transaction Detection

## ğŸ” Problem Statement

**PredCatch Analytics** serves leading Australian banks whose profitability and reputation are being severely affected by **fraudulent ATM transactions**. The company has tasked its data science team with developing a **real-time fraud detection model** that can proactively identify and decline suspicious transactions before they are processed.

Your role as a **PredCatch Data Scientist** is to:
- Build a robust predictive model to classify transactions as fraudulent or clean.
- Handle extreme class imbalance.
- Leverage additional proprietary and network-based variables to improve fraud detection accuracy.

## ğŸ¯ Project Objective

> Build an end-to-end machine learning pipeline to detect fraudulent ATM transactions using historical transaction metadata and advanced modeling techniques, ensuring **high recall**, **low false positives**, and **real-time readiness**.

## ğŸ—‚ï¸ Folder Structure

```bash
fraud-transaction-detector/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ processed/
â”‚   â”‚   â”œâ”€â”€ xtrain.csv
â”‚   â”‚   â”œâ”€â”€ xtest.csv
â”‚   â”‚   â””â”€â”€ ytrain.csv
â”‚   â””â”€â”€ raw/
â”œâ”€â”€ images/
â”‚   â””â”€â”€ *.png (visuals and box plots)
â”œâ”€â”€ models/
â”‚   â””â”€â”€ final_model.pkl
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ data-exploration.ipynb
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ preprocessing.py
â”‚   â”œâ”€â”€ modeling.py
â”‚   â””â”€â”€ utils.py
â”œâ”€â”€ venv/
â”œâ”€â”€ .gitignore
â”œâ”€â”€ main.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## ğŸ“Š Dataset Overview

The project uses **six main datasets** (renamed for clarity):
- `geo.csv`
- `instance.csv`
- `lambdawts.csv`
- `qset.csv`
- `train.csv`
- `test.csv`

These contain customer and transaction-level features including:
- Geographic location
- Proprietary fraud indices
- Vulnerability scores
- System network turnaround times

The **target** column in the train set:
- `1` = Fraudulent  
- `0` = Legitimate

## ğŸ§ª Data Exploration & Cleaning

âœ”ï¸ Loaded and reviewed all files  
âœ”ï¸ Inspected key fields like `id` and `group` for uniqueness and merging  
âœ”ï¸ Combined `train` and `test` datasets for uniform processing  
âœ”ï¸ Aggregated `geo`, `instance`, and `qset` data using `.groupby('id').mean()`  
âœ”ï¸ Handled shape mismatches by imputing missing values using **median imputation**  
âœ”ï¸ Successfully merged all data into one final dataset  
âœ”ï¸ Separated clean `train` and `test` sets and saved them in `/data/processed/`

## ğŸ§¹ Feature Engineering

- Dropped irrelevant fields: `id`, `group`, and `target` (after separation)
- Verified absence of null values
- Detected outliers using **boxplots** (but didnâ€™t treat them due to minimal impact)
- Visuals saved to `/images/`

## âš–ï¸ Class Imbalance

- The dataset had:
  - âœ… 227,451 normal transactions
  - âŒ 394 fraudulent transactions
- Severe imbalance addressed during modeling via careful metric selection and model tuning

## âš™ï¸ Data Standardization

- Standardization significantly improved model performance
- All final model training done using **standardized features**

## ğŸ¤– Models Implemented

| Model                  | Status         | Notes                                      |
|------------------------|----------------|--------------------------------------------|
| Logistic Regression    | âœ… Completed   | Performed well after standardization       |
| Decision Tree          | âœ… Completed   | Moderate performance                       |
| Random Forest          | âœ… Completed   | Solid baseline                             |
| XGBoost                | âœ… Completed   | Top performer                              |
| Support Vector Machine | âœ… Completed   | Improved with scaling                      |
| Voting Classifier      | âœ… Completed   | Ensemble of best models                    |
| Stacking Classifier    | âœ… Completed   | Final model used for deployment            |
| Isolation Forest       | âœ… Completed   | Unsupervised anomaly detection baseline    |

## ğŸ“ˆ Key Learnings

- Data **standardization** was crucial for performance
- Class imbalance required careful evaluation (focus on **recall**, **precision**, **AUC**)
- No outlier treatment was required due to limited effect
- Proper preprocessing pipelines and modular coding improved project clarity and maintainability

## ğŸ“‚ Output Files

- Final cleaned datasets in `/data/processed/`
- Visuals saved to `/images/`
- Trained model saved as `final_model.pkl` in `/models/`

## ğŸš€ How to Run

```bash
# Step 1: Install dependencies
pip install -r requirements.txt

# Step 2: Run the pipeline
python main.py
```

## ğŸ› ï¸ Future Improvements

- Integrate streaming fraud detection using Kafka or Spark
- Add SHAP/LIME for interpretability
- Use time-series modeling for evolving fraud patterns
- Implement cost-sensitive learning or focal loss

## ğŸ‘¨â€ğŸ’» Author

**Bikram Singh**  
Data Scientist | AI & Finance Enthusiast |  
ğŸ“§ Email: 1k99bikramsingh@gmail.com  
ğŸ”— LinkedIn: https://www.linkedin.com/in/1k99bikramsingh/